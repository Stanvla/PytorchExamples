{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import transformers as ppb\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings:\n",
      "           word_embeddings\n",
      "           position_embeddings\n",
      "           LayerNorm\n",
      "           dropout\n",
      "transformer:\n",
      "            layer:\n",
      "                  0:\n",
      "                    dropout\n",
      "                    attention:\n",
      "                              dropout\n",
      "                              q_lin\n",
      "                              k_lin\n",
      "                              v_lin\n",
      "                              out_lin\n",
      "                    sa_layer_norm\n",
      "                    ffn:\n",
      "                        dropout\n",
      "                        lin1\n",
      "                        lin2\n",
      "                    output_layer_norm\n",
      "                  1:\n",
      "                    dropout\n",
      "                    attention:\n",
      "                              dropout\n",
      "                              q_lin\n",
      "                              k_lin\n",
      "                              v_lin\n",
      "                              out_lin\n",
      "                    sa_layer_norm\n",
      "                    ffn:\n",
      "                        dropout\n",
      "                        lin1\n",
      "                        lin2\n",
      "                    output_layer_norm\n",
      "                  2:\n",
      "                    dropout\n",
      "                    attention:\n",
      "                              dropout\n",
      "                              q_lin\n",
      "                              k_lin\n",
      "                              v_lin\n",
      "                              out_lin\n",
      "                    sa_layer_norm\n",
      "                    ffn:\n",
      "                        dropout\n",
      "                        lin1\n",
      "                        lin2\n",
      "                    output_layer_norm\n",
      "                  3:\n",
      "                    dropout\n",
      "                    attention:\n",
      "                              dropout\n",
      "                              q_lin\n",
      "                              k_lin\n",
      "                              v_lin\n",
      "                              out_lin\n",
      "                    sa_layer_norm\n",
      "                    ffn:\n",
      "                        dropout\n",
      "                        lin1\n",
      "                        lin2\n",
      "                    output_layer_norm\n",
      "                  4:\n",
      "                    dropout\n",
      "                    attention:\n",
      "                              dropout\n",
      "                              q_lin\n",
      "                              k_lin\n",
      "                              v_lin\n",
      "                              out_lin\n",
      "                    sa_layer_norm\n",
      "                    ffn:\n",
      "                        dropout\n",
      "                        lin1\n",
      "                        lin2\n",
      "                    output_layer_norm\n",
      "                  5:\n",
      "                    dropout\n",
      "                    attention:\n",
      "                              dropout\n",
      "                              q_lin\n",
      "                              k_lin\n",
      "                              v_lin\n",
      "                              out_lin\n",
      "                    sa_layer_norm\n",
      "                    ffn:\n",
      "                        dropout\n",
      "                        lin1\n",
      "                        lin2\n",
      "                    output_layer_norm\n"
     ]
    }
   ],
   "source": [
    "# Loading a pretrained model\n",
    "# For DistilBERT, Load pretrained model/tokenizer:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "\n",
    "# recursive function to display model_structure\n",
    "def model_structure(layer, margin=0):\n",
    "    for name, next_layer in layer.named_children():\n",
    "        next = list(next_layer.named_children()) != []\n",
    "        print(' ' * margin + name + ':' * next)\n",
    "        model_structure(next_layer, margin + len(name) + 1)\n",
    "\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "model_structure(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   0  1\n0  a stirring , funny and finally transporting re...  1\n1  apparently reassembled from the cutting room f...  0\n2  they presume their audience wo n't sit still f...  0\n3  this is a visually stunning rumination on love...  1\n4  jonathan parker 's bartleby should have been t...  1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a stirring , funny and finally transporting re...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>apparently reassembled from the cutting room f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>they presume their audience wo n't sit still f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>this is a visually stunning rumination on love...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>jonathan parker 's bartleby should have been t...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apparently reassembled from the cutting room floor of any given daytime soap\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 4593,  2128, 27241, 23931,  2013,  1996,  6276,  2282,  2723,  1997,\n          2151,  2445, 12217,  7815]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization example\n",
    "print(df[0][1])\n",
    "tokenizer.encode(df[0][1], return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 5536\n",
      "Number of validation examples: 692\n",
      "Number of testing examples: 692\n"
     ]
    }
   ],
   "source": [
    "from data_preporation import  collate_fn, ReviewsDataset\n",
    "\n",
    "params = dict(\n",
    "    train_size=0.8,\n",
    "    val_size=0.1,\n",
    "    seed=0xDEAD,\n",
    "    batch=32,\n",
    "    hidden=256,\n",
    "    do=0.5,\n",
    "    lr=3e-5,\n",
    "    epochs=40,\n",
    "    clip=1,\n",
    "    save_fname='best_bert_sentiment.pt',\n",
    ")\n",
    "\n",
    "# dataset contains train/test/vat\n",
    "dataset = ReviewsDataset(df[0], tokenizer, df[1])\n",
    "\n",
    "# DON'T CHANGE, PLEASE\n",
    "torch.manual_seed(params['seed'])\n",
    "train_size, val_size = int(params['train_size'] * len(dataset)), int(params['val_size'] * len(dataset))\n",
    "train_data, valid_data, test_data = random_split(dataset, [train_size, val_size, len(dataset) - train_size - val_size])\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_sampler=ReviewsSampler(train_data, params['batch']), collate_fn=collate_fn)\n",
    "# valid_loader = DataLoader(valid_data, batch_sampler=ReviewsSampler(valid_data, params['batch']), collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(test_data, batch_sampler=ReviewsSampler(test_data, params['batch']), collate_fn=collate_fn)\n",
    "train_loader = DataLoader(train_data, batch_size=params['batch'], collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=params['batch'], collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=params['batch'], collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lightning model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "class BertClassifierPL(pl.LightningModule):\n",
    "    def __init__(self, pretrained_model, params):\n",
    "        super(BertClassifierPL, self).__init__()\n",
    "        self.bert = pretrained_model\n",
    "        self.params = params\n",
    "\n",
    "        self.dropout = nn.Dropout(p=params['do'])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.ln1 = nn.LazyBatchNorm1d()\n",
    "        self.fc = nn.Linear(768, params['hidden'])\n",
    "        self.ln2 = nn.LazyBatchNorm1d()\n",
    "        self.fc_out = nn.Linear(params['hidden'], 1)\n",
    "\n",
    "    def forward(self, inputs, attention_mask):\n",
    "        # [batch_size, seq_len]\n",
    "        hidden_states = self.bert(inputs, attention_mask=attention_mask)[0]\n",
    "        # [batch_size, seq_len, bert_hidden_size]\n",
    "\n",
    "        hidden_states = hidden_states.mean(dim=1)\n",
    "        # [batch_size, bert_hidden_size]\n",
    "\n",
    "        hidden_states = self.fc(self.dropout(self.relu(self.ln1(hidden_states))))\n",
    "        outputs = self.fc_out(self.ln2(hidden_states))\n",
    "        # [batch_size, 1]\n",
    "\n",
    "        # proba = [batch_size, ] - probability to be positive\n",
    "        return self.sigmoid(outputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels, mask = batch['inputs'], batch['labels'], batch['attention_mask']\n",
    "\n",
    "        output = self(inputs, mask).squeeze()\n",
    "        loss = F.binary_cross_entropy(output, labels)\n",
    "\n",
    "        predictions = (output >= 1/2) + 0\n",
    "        labels = labels.type(torch.cuda.IntTensor)\n",
    "        acc = accuracy(predictions, labels)\n",
    "\n",
    "        return dict(loss=loss, acc=acc, batch_size=inputs.shape[0])\n",
    "\n",
    "    # exactly as training step but can be different\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels, mask = batch['inputs'], batch['labels'], batch['attention_mask']\n",
    "\n",
    "        output = self(inputs, mask).squeeze()\n",
    "        loss = F.binary_cross_entropy(output, labels)\n",
    "\n",
    "        predictions = (output >= 1/2) + 0\n",
    "        labels = labels.type(torch.cuda.IntTensor)\n",
    "        acc = accuracy(predictions, labels)\n",
    "\n",
    "        return dict(val_loss=loss, val_acc=acc, batch_size=inputs.shape[0])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.params['lr'])\n",
    "\n",
    "        # default configuration for scheduler\n",
    "\n",
    "        # When there are schedulers in which the .step() method is conditioned on a value,\n",
    "        # such as the torch.optim.lr_scheduler.ReduceLROnPlateau scheduler,\n",
    "        # Lightning requires that the lr_scheduler_config contains the keyword \"monitor\"\n",
    "        # set to the metric name that the scheduler should be conditioned on.\n",
    "\n",
    "        # lr_scheduler_config = {\n",
    "        #      # REQUIRED: The scheduler instance\n",
    "        #     \"scheduler\": lr_scheduler,\n",
    "        #\n",
    "        #      # The unit of the scheduler's step size, could also be 'step'.\n",
    "        #      # 'epoch' updates the scheduler on epoch end whereas 'step'\n",
    "        #      # updates it after a optimizer update.\n",
    "        #     \"interval\": \"epoch\",\n",
    "        #\n",
    "        #      # How many epochs/steps should pass between calls to\n",
    "        #      # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "        #      # rate after every epoch/step.\n",
    "        #     \"frequency\": 1,\n",
    "        #\n",
    "        #      # Metric to monitor for schedulers like `ReduceLROnPlateau`\n",
    "        #     \"monitor\": \"val_loss\",\n",
    "        #\n",
    "        #      # If set to `True`, will enforce that the value specified 'monitor'\n",
    "        #      # is available when the scheduler is updated, thus stopping\n",
    "        #      # training if not found. If set to `False`, it will only produce a warning\n",
    "        #     \"strict\": True,\n",
    "        #\n",
    "        #      # If using the `LearningRateMonitor` callback to monitor the\n",
    "        #      # learning rate progress, this keyword can be used to specify\n",
    "        #      # a custom logged name\n",
    "        #     \"name\": None,\n",
    "        # }\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            # lr_scheduler=lr_scheduler_config,\n",
    "        )\n",
    "\n",
    "    # # ....................... hooks ...............................\n",
    "    def avg_output(self, loss_key, acc_key, outputs):\n",
    "        losses, accs, total = [], [], 0\n",
    "        for x in outputs:\n",
    "            total += x['batch_size']\n",
    "            losses.append(x[loss_key] * x['batch_size'])\n",
    "            accs.append(x[acc_key] * x['batch_size'])\n",
    "        return torch.stack(losses).sum() / total, torch.stack(accs).sum() / total\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_loss, val_acc = self.avg_output('val_loss', 'val_acc', outputs)\n",
    "        self.logger.experiment.add_scalar('Loss/Val', val_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Acc/Val', val_acc, self.current_epoch)\n",
    "        self.log('val_acc', val_acc, logger=False)\n",
    "        self.log('val_loss', val_loss,logger=False)\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        train_loss, train_acc = self.avg_output('loss', 'acc', outputs)\n",
    "        self.logger.experiment.add_scalar('Loss/Train', train_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Acc/Train', train_acc, self.current_epoch)\n",
    "\n",
    "    # def test_step(self):\n",
    "    #     pass\n",
    "    #\n",
    "    # def train_dataloader(self):\n",
    "    #     pass\n",
    "    #\n",
    "    # def val_dataloaders(self):\n",
    "    #     pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMisconfigurationException\u001B[0m                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-bcfba5b37c13>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0;31m# multiple GPUs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m trainer = pl.Trainer(gpus=2, strategy='ddp', logger=logger, num_sanity_val_steps=0, max_epochs=6, callbacks=[loss_checkpoint_callback, acc_checkpoint_callback],\n\u001B[0;32m---> 67\u001B[0;31m                      enable_checkpointing=True)\n\u001B[0m\u001B[1;32m     68\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[0mpl_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBertClassifierPL\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001B[0m in \u001B[0;36minsert_env_defaults\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0;31m# all args were already moved to kwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0minsert_env_defaults\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001B[0m\n\u001B[1;32m    446\u001B[0m             \u001B[0mamp_backend\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    447\u001B[0m             \u001B[0mamp_level\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 448\u001B[0;31m             \u001B[0mplugins\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    449\u001B[0m         )\n\u001B[1;32m    450\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogger_connector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mLoggerConnector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlog_gpu_memory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, num_processes, devices, tpu_cores, ipus, accelerator, strategy, gpus, gpu_ids, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 164\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_training_type_plugin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    165\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_distributed_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001B[0m in \u001B[0;36m_set_training_type_plugin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    309\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_training_type_plugin\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTrainingTypePluginsRegistry\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_distributed_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTrainingTypePlugin\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_training_type_plugin\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001B[0m in \u001B[0;36mset_distributed_mode\u001B[0;34m(self, strategy)\u001B[0m\n\u001B[1;32m    900\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    901\u001B[0m         \u001B[0;31m# finished configuring self._distrib_type, check ipython environment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 902\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_interactive_compatibility\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    903\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m         \u001B[0;31m# for DDP overwrite nb processes by requested GPUs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001B[0m in \u001B[0;36mcheck_interactive_compatibility\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    942\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_IS_INTERACTIVE\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_distrib_type\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_distrib_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_interactive_compatible\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    943\u001B[0m             raise MisconfigurationException(\n\u001B[0;32m--> 944\u001B[0;31m                 \u001B[0;34mf\"`Trainer(strategy={self._distrib_type.value!r})` or\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    945\u001B[0m                 \u001B[0;34mf\" `Trainer(accelerator={self._distrib_type.value!r})` is not compatible with an interactive\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    946\u001B[0m                 \u001B[0;34m\" environment. Run your code as a script, or choose one of the compatible backends:\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mMisconfigurationException\u001B[0m: `Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.html#module-pytorch_lightning.trainer.trainer\n",
    "# check the model is working\n",
    "# fast_dev_run ... pass one batch through the model, if false normal training with whole data\n",
    "# trainer = pl.Trainer(gpus=1, fast_dev_run=True)\n",
    "\n",
    "def version_from_params(p, shortcuts):\n",
    "    return '__'.join([f'{short}_{p[orig]}' for orig, short in shortcuts.items()])\n",
    "\n",
    "shortcuts = dict(\n",
    "    batch='bs',\n",
    "    hidden='hd',\n",
    "    do='do',\n",
    "    lr='lr',\n",
    "    epochs='ep',\n",
    ")\n",
    "# time should not contain double colon\n",
    "time = datetime.now().strftime('%d.%m__%H.%M')\n",
    "logs_dir = 'logs'\n",
    "experiment_name = 'sent_analysis'\n",
    "logger = TensorBoardLogger(logs_dir, name=experiment_name, version=version_from_params(params, shortcuts), sub_dir=time)\n",
    "path = os.path.join(logs_dir, experiment_name, version_from_params(params, shortcuts), time, 'checkpoints')\n",
    "# ic(path)\n",
    "loss_checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=path,\n",
    "    filename='{epoch:02d}--{val_loss:.2f}',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "acc_checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",\n",
    "    dirpath=path,\n",
    "    filename='{epoch:02d}--{val_acc:.2f}',\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "# single GPU, the only possible way of training in jupyter-notebook\n",
    "trainer = pl.Trainer(gpus=1, logger=logger, num_sanity_val_steps=0, max_epochs=5, callbacks=[loss_checkpoint_callback, acc_checkpoint_callback], enable_checkpointing=True)\n",
    "# trainer = pl.Trainer(gpus=2, strategy='ddp', logger=logger, num_sanity_val_steps=0, max_epochs=6, callbacks=[loss_checkpoint_callback, acc_checkpoint_callback],\n",
    "#                      enable_checkpointing=True)\n",
    "\n",
    "pl_model = BertClassifierPL(model, params)\n",
    "trainer.fit(pl_model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# tensorboard --logdir NLP/BERT_sent_analysis/lightning_logs --port 6006\n",
    "# !rm -rf checkpoints\n",
    "# !echo '{path}'\n",
    "!ls '{path}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_text_classification.ipynb\tPytorchLightning.ipynb\tdata_preporation.py\r\n",
      "PurePytorch.ipynb\t\t__pycache__\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}